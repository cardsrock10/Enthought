{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last updated: Jan 5th 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate data exploration: a journey through Pandas: part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the part 1, we (down)loaded datasets for temperature, and sea level measurements, cleaned them up and visualized them. In this second part, we will analyze these datasets and get to answer the questions we started with:\n",
    "  - How correlated are the global temperature, sea levels and greenhouse gaz emission (of course, correlation doesn't prove causation)?\n",
    "  - What temperature and sea level rise should we expect for the end of the century in New York (or any other coastal city in the world you are interested in)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process, we will learn about:\n",
    "\n",
    "Part 1:\n",
    "\n",
    "    1. Loading data\n",
    "    2. Pandas datastructures\n",
    "    3. Cleaning and formatting data\n",
    "    4. Statistical description of data \n",
    "    5. Basic visualization\n",
    "    6. Exporting data to files\n",
    "    \n",
    "Part 2:\n",
    "    \n",
    "    7. Accessing data\n",
    "    8. Working with dates and times\n",
    "    9. Transforming datasets\n",
    "    10. Data agregation and summarization\n",
    "    11. Correlations and regressions\n",
    "    12. Predictions from auto regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "print \"Pandas version:\", pd.__version__\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This allows to control a lot of styling options, which might be\n",
    "# quite critical when working with data:\n",
    "pd.set_option(\"display.max_rows\", 16)\n",
    "\n",
    "# This will be useful to blow matplotlib figures larger:\n",
    "LARGE_FIGSIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reload the clean data that we stored in part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If part1 failed to generate a clean dataset, load the backup file:\n",
    "# store = pd.HDFStore(\"data/cleaned_climate_data_BACKUP.h5\")\n",
    "store = pd.HDFStore(\"cleaned_climate_data.h5\", \"r\")\n",
    "mean_sea_level = pd.read_hdf(store, \"/sea_level/timeseries\")\n",
    "local_sea_level_stations = pd.read_hdf(store, \"/sea_level/station_data\")\n",
    "giss_temp = pd.read_hdf(store, \"/temperature/monthly\")\n",
    "full_globe_temp = pd.read_hdf(store, \"/temperature/yearly\")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## If there are problems with Pandas and the PyTables package, load\n",
    "## results from backup CSV files instead.\n",
    "# mean_sea_level = pd.read_csv(\"data/clean_data/sea_level.csv\", index_col=0)\n",
    "# local_sea_level_stations = pd.read_csv(\"data/clean_data/sea_level_station_data.csv\", index_col=0)\n",
    "# giss_temp = pd.read_csv(\"data/clean_data/temperature_monthly.csv\", index_col=0)\n",
    "# full_globe_temp = pd.read_csv(\"data/clean_data/temperature_yearly.csv\", index_col=0, \n",
    "#                               parse_dates=True, squeeze=True, header=None, names=['year', 'mean temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Accessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/indexing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general philosophy for accessing values inside a Pandas datastructure is that, unlike a numpy array that only allows to index using integers a Series allows to index with the values inside the index. That makes the code more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `[]` on a series accesses values using the index, not the location in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = pd.Series(range(5), index=list(\"abcde\"))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s[\"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the sea level rise in 1880?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This index is non-trivial (will talk more about these datetime \n",
    "# objects further down):\n",
    "full_globe_temp.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily make a `Timestamp` object from a string, one can use the `pd.to_datetime()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.to_datetime('1880')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By default [] on a series accesses values using the index, \n",
    "# not the location in the series\n",
    "print(full_globe_temp[pd.to_datetime('1880')])\n",
    "# print(temp1[0])  # This would fail!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another more explicit way to do the same thing is to use loc\n",
    "print(full_globe_temp.loc[pd.to_datetime('1990')])\n",
    "print(full_globe_temp.iloc[0], full_globe_temp.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Year of the last record?\n",
    "full_globe_temp.index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas datastructures are extensible, new records can be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_globe_temp[pd.to_datetime('2011')] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2D, same idea, though in a DF `[]` accesses columns (Series) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp[\"Jan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more complete queries, `.loc` and `.iloc` should be used: they are faster and less ambiguous to access individual values, slices or masked selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp.loc[1979, \"Dec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slicing can be done with .loc and .iloc\n",
    "print(giss_temp.loc[1979, \"Jan\":\"Jun\"])  # Note that the end point is included unlike NumPy!!!\n",
    "print(giss_temp.loc[1979, ::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking can also be used in one or more dimensions. For example, another way to grab every other month for the first year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = [True, False] * 6\n",
    "print(giss_temp.iloc[0, mask])\n",
    "print(giss_temp.loc[1880, mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a series, we could also add a new column like a new entry in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp[\"totals\"] = giss_temp.sum(axis=1)\n",
    "giss_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove this new column, we will learn to do this differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp = giss_temp.drop(\"totals\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex queries rely on the same concepts. For example what are the names, and IDs of the sea level stations in the USA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "american_stations = local_sea_level_stations[\"Country\"] == \"USA\"\n",
    "american_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations.loc[american_stations, [\"ID\", \"Station Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** What are the countries of the stations in latitudes above 60 and below -60. Print each country only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with dates and times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details at http://pandas.pydata.org/pandas-docs/stable/timeseries.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parsed the dates to build the `full_globe_temp` Series. Let's see what that allows us to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Its dtype is NumPy's new 'datetime64[ns]':\n",
    "full_globe_temp.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of having a real datetime index is that operations can be done efficiently on it. Let's add a flag to signal if the value is before or after the great depression's black Friday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "black_friday = pd.to_datetime('1929-10-29')\n",
    "full_globe_temp.index > black_friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps or periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert its index from timestamp to period: it is more meaningfull \n",
    "# since it was measured and averaged over the year...\n",
    "full_globe_temp.index = full_globe_temp.index.to_period()\n",
    "full_globe_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also `to_timestamp` to conver back to timestamps and its `how` method to specify when inside the range to set the timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that can be done is to resample the series, downsample or upsample. Let's see the series converted to 10 year blocks or upscale to a monthly series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequencies can be specified as strings: \"us\", \"ms\", \"S\", \n",
    "# \"T\", \"H\", \"D\", \"B\", \"W\", \"M\", \"A\", \"3min\", \"2h20\", ...\n",
    "# More aliases at http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "full_globe_temp.resample(\"M\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_globe_temp.resample(\"10A\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating `DatetimeIndex` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index for `giss_temp` isn't an instance of datetimes so we may want to generate such `DatetimeIndex` objects. This can be done with `date_range` and `period_range`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can specify a start date and a number of values desired. By default \n",
    "# it will assume an interval of 1 day:\n",
    "pd.date_range('1/1/1880', periods=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can also specify a start and a stop date, as well as a frequency\n",
    "pd.date_range('1/1/1880', '1/1/2016', freq=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `\"A\"` by default means the end of the year. Other times in the year can be specified with `\"AS\"` (start), `\"A-JAN\"` or `\"A-JUN\"`. Even more options can be imported from `pandas.tseries.offsets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import YearBegin\n",
    "pd.date_range('1/1/1880', '1/1/2015', freq=YearBegin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we will convert that dataset to a 1D dataset, and build a monthly index, so lets build a monthly period index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp_index = pd.period_range('1/1/1880', '04/1/2015', freq=\"M\")\n",
    "giss_temp_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transforming datasets: rolling stats, custom transformations, sort and reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/computation.html and http://pandas.pydata.org/pandas-docs/stable/reshaping.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove high frequency signal and extract the trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_globe_temp.plot()\n",
    "full_globe_temp.rolling(window=10).mean().plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Refer to `exercises/pandas/pandas_moving_average/pandas_moving_average.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our `local_sea_level_stations` dataset some more, to learn more about it and also do some formatting. What is the range of dates and lattitudes we have, the list of countries, the range of variations, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What about the range of dates?\"\n",
    "local_sea_level_stations[\"Date\"].min(), local_sea_level_stations[\"Date\"].max(), local_sea_level_stations[\"Date\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply: transforming Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see the correct range of dates because the dates are of dtype \"Object\", (usually meaning strings). Let's convert that using `apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations[\"Date\"].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `apply` method is very powerful and general. We have used it to do something we could have done with `astype`, but any custom function can be provided to `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations[\"Date\"] = local_sea_level_stations[\"Date\"].apply(pd.to_datetime)\n",
    "\n",
    "# Now we can really compare the dates, and therefore get a real range:\n",
    "print(local_sea_level_stations[\"Date\"].min(), local_sea_level_stations[\"Date\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the range of dates, to look at the data, sorting it following the dates is done with `sort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations.sort_values(by=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many stations last updated on the same dates, it is logical to want to sort further, for example, by `Country` at constant date, and then by station name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations.sort_values(by=[\"Date\", \"Country\", \"Station Name\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Use the `apply` method to search through the stations names for a station in New York. What is the ID of the station?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack and unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the GISS dataset differently. Instead of seeing the months along the axis 1, and the years along the axis 0, it would could be good to convert these into an outer and an inner axis along only 1 time dimension.\n",
    "\n",
    "Stacking and unstacking allow to convert pandas structures from wide formats to long format. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp.stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp_series = giss_temp.stack()\n",
    "giss_temp_series.name = \"Temp anomaly\"\n",
    "giss_temp_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note the nature of the result:\n",
    "type(giss_temp_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A side note: Multi-indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/advanced.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note the nature of the resulting index:\n",
    "giss_temp_series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is an index made of 2 columns. Let's fix the fact that one of them doesn't have a name:\n",
    "giss_temp_series.index = giss_temp_series.index.set_names([\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can now access deviations by specifying the year and month:\n",
    "giss_temp_series[(1980, \"Jan\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this new multi-index isn't very good, because is it not viewed as 1 date, just as a tuple of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp_series.plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve on this, let's reuse an index we generated above with `period_range`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp_series.index = giss_temp_index\n",
    "giss_temp_series.plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data agregation and summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good grasp on our datasets, Let's transform and analyze them some more to prepare them to compare them. The 3 function(alities)s to learn about here are `groupby`, `merge` and `pivot_table`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/groupby.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the average lattitude of sea level stations for each country in `local_sea_level_stations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by an existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_stations_grouped_country = local_sea_level_stations.groupby(\"Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of object did we create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(sl_stations_grouped_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do with that strange GroupBy object? We can first loop over it to get the labels and the sub-dataframes for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for group_name, subdf in sl_stations_grouped_country:\n",
    "    print(group_name)\n",
    "    print(subdf)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each subdf contains all the series since it is a `DataFrameGroupBy` object. In fact the groupby object can be reduced to one of these series with regular index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_stations_grouped_country[\"Lat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that allows us to compute the mean latitudes, by applying the aggregation function to that sub object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_stations_grouped_country[\"Lat\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping by the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the sea levels, splitting into calendar years to compute average sea levels for each year. That also shows that we can do the groupby on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_grouped_year = mean_sea_level.groupby(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something else that can be done with such an object is to look at its `groups` attribute to see the labels mapped to the rows involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_grouped_year.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to aggregate the results of this grouping depends on what we want to see: do we want to see averaged over the years? That is so common that it has been implemented directly as a method on the `GroupBy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_grouped_year.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can apply any other reduction function or even a dict of functions \n",
    "# using aggregate:\n",
    "sl_grouped_year.aggregate({\"mean_global\": np.std})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `GroupBy` to transform different parts of a DF differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to transform each group separately, rather than aggregate. For example, here we group the mean sea levels over decades and subtract to each value the average over that decade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_grouped_decade = mean_sea_level.groupby(lambda x: int(x/10.))\n",
    "sl_grouped_decade.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(subframe):\n",
    "    return (subframe - subframe.mean()) / subframe.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_grouped_decade.transform(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-type `join`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to add more information to our station table. In particular, we would like to add a column of continents, so that we can group data by continent. That can be done using a table of country code and continent code found at http://dev.maxmind.com/geoip/legacy/codes/country_continent/. Unfortunately, the country codes we have are 3-letter but the mapping between 3-letter and 2-letter code can be found in https://en.wikipedia.org/wiki/ISO_3166-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the 2 tables that together contain all the information needed. The country_continent table is stored locally in the `data/misc/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country2_continent = pd.read_csv(\"data/misc/country_continent.csv\", \n",
    "                                 na_values=[\"--\"], keep_default_na=False)\n",
    "country2_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's check we have found stations on all continents\n",
    "country2_continent[\"continent code\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz**: What happens if `keep_default_na` is not set in the `read_csv` above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Local backup:\n",
    "# country_codes = pd.read_excel(\"data/misc/country_codes.xlsx\")\n",
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/ISO_3166-1\")\n",
    "country_codes = tables[0]\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** The table's headers are found in the first row. Make them the proper headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't look! solution below...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_headers = list(country_codes.iloc[0])\n",
    "country_codes.columns = new_headers\n",
    "country_codes = country_codes.drop(0)\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these 2 tables, let's first make a table with the 3-letter county codes together with the continents. That's a `join` operation which can be done in Pandas with `pd.merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_code_cols = [\"English short name (upper/lower case)\", \n",
    "                     \"Alpha-2 code\", \"Alpha-3 code\"]\n",
    "raw_mapping = pd.merge(country_codes[country_code_cols], \n",
    "                       country2_continent, \n",
    "                       how=\"outer\", \n",
    "                       left_on=\"Alpha-2 code\", \n",
    "                       right_on=\"iso 3166 country\")\n",
    "raw_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really, all we are after is the mapping between the 3-letter code and the continent, so let's drop everything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = raw_mapping[[\"Alpha-3 code\", \"continent code\"]].dropna(how=\"any\")\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(local_sea_level_stations, mapping, how=\"left\", \n",
    "                  left_on=\"Country\", right_on=\"Alpha-3 code\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations = merged.drop(\"Alpha-3 code\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use `groupby` to compute the number of stations in each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see http://pandas.pydata.org/pandas-docs/stable/reshaping.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot table also allows to summarize the information, allowing to convert repeating column values into dimensions of a new table. \n",
    "\n",
    "For example, let's say that we would like to know how many sea level stations are on various continents. And we would like to group the answers into 2 categories: the stations that have been updated recently (after `2000/1/1`) and the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Thanks to the previous merge, we already have a column with the (repeating) continent codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations[\"continent code\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values will become the index of our future table. The columns of our future table should have 2 values, whether the station was updated recently or not. Let's build a column to store that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations[\"Recently updated\"] = local_sea_level_stations[\"Date\"] > pd.to_datetime(\"2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, what value will be displayed inside the table? The values should be extracted from a column, `pivot_table` allowing an aggregation function to be applied when more than 1 value is found for a given case. Each station should count for 1, and we could aggregate multiple stations by summing these 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_sea_level_stations[\"Number of stations\"] = np.ones(len(local_sea_level_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_counts = pd.pivot_table(local_sea_level_stations, \n",
    "                                index=\"Recently updated\", \n",
    "                                columns=\"continent code\", \n",
    "                                values=\"Number of stations\", \n",
    "                                aggfunc=np.sum)\n",
    "station_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE (Optional):** How would we build the same dataframe with a `groupby` operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** \n",
    "  1. Build a similar table but with the `recently updated` information along the columns, and all country codes as rows. \n",
    "  2. How many recently updated stations is there in the world? Not recently updated stations? Look at the documentation for `pd.pivot_table`'s margins option. Which country has the most stations?\n",
    "  3. Build another table showing the `recently updated` information along the columns, and all continents as rows but with both the average latitude and average longitude as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Refer to `exercises/pandas/pandas_dataframe/pandas_dataframe.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Correlations and regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Series and dataframes have a `corr` method to compute the correlation coefficient between series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see what how the various sea levels are correlated with each other:\n",
    "mean_sea_level[\"northern_hem\"].corr(mean_sea_level[\"southern_hem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If series are already grouped into a DataFrame, computing all \n",
    "# correlation coeff is trivial:\n",
    "mean_sea_level.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: by default, the method used is the `Pearson` correlation coefficient. Other methods are available (`kendall`, `spearman` using the `method` kwarg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix\n",
    "plt.imshow(mean_sea_level.corr(), interpolation=\"nearest\", cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's make it a little better to confirm that learning about global sea level cannot be done from just \n",
    "# looking at stations in the northern hemisphere:\n",
    "plt.imshow(mean_sea_level.corr(), interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.xticks(np.arange(3), mean_sea_level.corr().columns)\n",
    "plt.yticks(np.arange(3), mean_sea_level.corr().index)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series modelling with `OLS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas's ability to build Ordinary Least Square modelling relies on the `statsmodels` package. \n",
    "\n",
    "OLS in pandas requires to pass a `y` series and an `x` series to do a fit of the form `y ~ x`. But the formula can be more complex by providing a `DataFrame` for x and reproduce a formula of the form `y ~ x1 + x2`. \n",
    "\n",
    "Also possible are rolling and expanding OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.stats.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ols(y=mean_sea_level[\"mean_global\"], \n",
    "            x=mean_sea_level[[\"northern_hem\", \"southern_hem\"]])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary_as_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=LARGE_FIGSIZE)\n",
    "mean_sea_level[\"mean_global\"].plot()\n",
    "model.predict().plot(label=\"OLS prediction\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An interlude: data alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the floating point date to a timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to look for correlations between our monthly temperatures and the sea levels we have. For this to be possible, some data alignment must be done since the time scales are very different for the 2 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_sea_level[\"mean_global\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giss_temp_series.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a function that converts the floating point dates in the sea level to timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "def floating_year_to_timestamp(float_date):\n",
    "    \"\"\" Convert a date as a floating point year number to a pandas timestamp object.\n",
    "    \"\"\"\n",
    "    year = int(float_date)\n",
    "    days_per_year = 366 if calendar.isleap(year) else 365\n",
    "    remainder = float_date - year\n",
    "    daynum = 1 + remainder * (days_per_year - 1)\n",
    "    daynum = int(round(daynum))\n",
    "    \n",
    "    # Convert day number to month and day\n",
    "    day = daynum\n",
    "    month = 1\n",
    "    while month < 13:\n",
    "        month_days = calendar.monthrange(year, month)[1]\n",
    "        if day <= month_days:\n",
    "            return pd.to_datetime(str(year)+\"/\"+str(month)+\"/\"+str(day))\n",
    "        day -= month_days\n",
    "        month += 1\n",
    "    raise ValueError('{} does not have {} days'.format(year, daynum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_index = pd.Series(mean_sea_level[\"mean_global\"].index).apply(floating_year_to_timestamp)\n",
    "dt_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_sea_level = mean_sea_level.reset_index(drop=True)\n",
    "mean_sea_level.index = dt_index\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to align the 2 series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning `Series` and `DataFrame`s with the `align` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's downscale the frequency of the sea levels to monthly, like the temperature reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_mean_sea_level = mean_sea_level.resample(\"MS\").mean().to_period()\n",
    "monthly_mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the series are using the same type and frequency of indexes, to align them is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_mean_sea_level[\"mean_global\"].align(giss_temp_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the alignment leads to the *union* of the 2 indexes. We would rather have the intersection to avoid so much missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_mean_sea_level[\"mean_global\"].align(giss_temp_series, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store that result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_sl, aligned_temp = monthly_mean_sea_level[\"mean_global\"].align(giss_temp_series, \n",
    "                                                                       join='inner')\n",
    "aligned_df = pd.DataFrame({\"mean_sea_level\": aligned_sl, \"mean_global_temp\": aligned_temp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alignment can even be done on an entire dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthly_mean_sea_level.align(giss_temp_series, axis=0, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_sea_levels, aligned_temp = monthly_mean_sea_level.align(giss_temp_series, axis=0, join='inner')\n",
    "aligned_monthly_data = aligned_sea_levels.copy()\n",
    "aligned_monthly_data[\"global_temp\"] = aligned_temp\n",
    "aligned_monthly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between sea levels and temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_monthly_data.plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_monthly_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ols(y=aligned_monthly_data[\"southern_hem\"], \n",
    "            x=aligned_monthly_data[\"global_temp\"])\n",
    "model.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had done the analysis yearly instead of monthly to remove seasonal variations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_yearly_data = aligned_monthly_data.resample(\"A\").mean()\n",
    "aligned_yearly_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aligned_yearly_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ols(y=aligned_yearly_data[\"southern_hem\"], \n",
    "            x=aligned_yearly_data[\"global_temp\"])\n",
    "model.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (Optional)**: If not already done, load the greenhouse gaz data located at `data/greenhouse_gaz/co2_mm_global.txt` and add it to the `aligned_monthly_data` dataframe. Use that to plot the greenhouse data together with the other curves for sea level and temperature. How well is this dataset correlated with temperature and sea level data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Predictions from auto regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auto-regresssive model fits existing data and builds a (potentially predictive) model of the data fitted. We use the timeseries analysis (`tsa`) submodule of `statsmodels` to make out-of-sample predictions for the upcoming decades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "# Let's remove seasonal variations by resampling annually\n",
    "data = giss_temp_series.resample(\"A\").mean().to_timestamp()\n",
    "ar_model = sm.tsa.ar_model.AR(data, freq='A')\n",
    "ar_res = ar_model.fit(maxlag=60, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=LARGE_FIGSIZE)\n",
    "pred = ar_res.predict(start='1950-1-1', end='2070')\n",
    "data.plot(style='k', label=\"Historical Data\")\n",
    "pred.plot(style='r', label=\"Predicted Data\")\n",
    "plt.ylabel(\"Temperature variation (0.01 degC)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Make another auto-regression on the sea level of the Atlantic ocean to estimate how much New York is going to flood in the coming century. To do that, extract the ID of a station in NewYork from the `local_sea_level_stations` dataset, and use it to download timeseries in NY. Reminder: the URL to build is for the form http://www.psmsl.org/data/obtaining/met.monthly.data/[ID].metdata once the ID is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to practice more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE (computations):** Refer to `exercises/stock_returns/stock_returns.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE (stats, groupby, timeseries):** Refer to `exercises/pandas_wind_statistics/pandas_wind_statistics.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
